{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44145de0",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20d28036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5cdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Dataset.csv')\n",
    "texts = df['clause'].tolist()\n",
    "labels = df['risk'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9084b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a21d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64  # or any other suitable length\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
    "\n",
    "# Truncate and pad sequences\n",
    "padded_sequences = []\n",
    "for seq in tokenized_texts:\n",
    "    if len(seq) > max_length:\n",
    "        # Truncate the sequence if it exceeds max_length\n",
    "        seq = seq[:max_length]\n",
    "    else:\n",
    "        # Pad the sequence if it's shorter than max_length\n",
    "        seq = seq + [tokenizer.pad_token_id] * (max_length - len(seq))\n",
    "    padded_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23bdf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.LongTensor(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f40ee4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 'labels' for your texts\n",
    "dataset = TextClassificationDataset(padded_sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f1e1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels)\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97d79507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27dc2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# Use GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be1e164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze GPT model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be3e3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add classification head\n",
    "# class GPTClassifier(nn.Module):\n",
    "#     def __init__(self, gpt_model):\n",
    "#         super(GPTClassifier, self).__init__()\n",
    "#         self.gpt = gpt_model\n",
    "#         self.dropout = nn.Dropout(0.5)  # Add dropout layer with dropout rate 0.5\n",
    "#         self.fc = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         outputs = self.gpt(input_ids)[0]\n",
    "#         pooled_output = outputs[:, :, 0]  # Take the first token [CLS]\n",
    "#         pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "#         logits = self.fc(pooled_output)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Add classification head\n",
    "# class GPTClassifier(nn.Module):\n",
    "#     def __init__(self, gpt_model):\n",
    "#         super(GPTClassifier, self).__init__()\n",
    "#         self.gpt = gpt_model\n",
    "#         self.fc = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         outputs = self.gpt(input_ids)[0]\n",
    "\n",
    "#         pooled_output = outputs[:,:,0]  # Take the first token [CLS]\n",
    "#         logits = self.fc(pooled_output)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, gpt_model, dropout_prob=0.5):\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.gpt = gpt_model\n",
    "        self.fc = nn.Linear(25000, 1)  # Assuming hidden_state dimension is 25000\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.gpt(input_ids)[0]  # Getting hidden states\n",
    "        pooled_output = outputs[:, 0, :]  # Take the first token [CLS] \n",
    "        # print(\"pooled_output.shape\", pooled_output.shape)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # input_ids, labels = batch\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            predictions = (logits > 0).long()\n",
    "            correct_train += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, labels = batch\n",
    "                logits = model(input_ids)\n",
    "                loss = criterion(logits.squeeze(-1), labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                predictions = (logits > 0).long()\n",
    "                correct_val += (predictions == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        # Print training and validation statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "        # Store losses and accuracies for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "    \n",
    "\n",
    "def plot_val_acc_per_epoch(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "    # Plot losses\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.plot(train_accuracies, label='Train Acc')\n",
    "    plt.plot(val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d57dedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier = GPTClassifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "416358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(classifier, train_dataloader, test_dataloader, optimizer, criterion, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_acc_per_epoch(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8afce903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([14, 128])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the test dataset and generate predictions\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        logits = classifier(input_ids)\n",
    "        predictions = (logits > 0).long()  # Convert logits to binary predictions\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "59e62e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (with 4 decimal points precision):\n",
      "[[313   7]\n",
      " [203   3]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the confusion matrix with 4 decimal points precision\n",
    "print(\"Confusion Matrix (with 4 decimal points precision):\")\n",
    "print(np.round(conf_matrix, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8a13c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.98      0.75       320\n",
      "         1.0       0.30      0.01      0.03       206\n",
      "\n",
      "    accuracy                           0.60       526\n",
      "   macro avg       0.45      0.50      0.39       526\n",
      "weighted avg       0.49      0.60      0.47       526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 has 292 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (25000, 1024)\n",
      "transformer.wpe.weight                                   (256, 1024)\n",
      "\n",
      "==== 1st Transformer Layer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                  (1024,)\n",
      "transformer.h.0.ln_1.bias                                    (1024,)\n",
      "transformer.h.0.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.0.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.0.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.0.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.0.ln_2.weight                                  (1024,)\n",
      "transformer.h.0.ln_2.bias                                    (1024,)\n",
      "transformer.h.0.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.0.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.0.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.0.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== 2nd Transformer Layer ====\n",
      "\n",
      "transformer.h.1.ln_1.weight                                  (1024,)\n",
      "transformer.h.1.ln_1.bias                                    (1024,)\n",
      "transformer.h.1.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.1.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.1.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.1.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.1.ln_2.weight                                  (1024,)\n",
      "transformer.h.1.ln_2.bias                                    (1024,)\n",
      "transformer.h.1.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.1.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.1.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.1.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== 3rd Transformer Layer ====\n",
      "\n",
      "transformer.h.2.ln_1.weight                                  (1024,)\n",
      "transformer.h.2.ln_1.bias                                    (1024,)\n",
      "transformer.h.2.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.2.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.2.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.2.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.2.ln_2.weight                                  (1024,)\n",
      "transformer.h.2.ln_2.bias                                    (1024,)\n",
      "transformer.h.2.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.2.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.2.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.2.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.h.23.mlp.c_fc.bias                               (4096,)\n",
      "transformer.h.23.mlp.c_proj.weight                      (4096, 1024)\n",
      "transformer.h.23.mlp.c_proj.bias                             (1024,)\n",
      "transformer.ln_f.weight                                      (1024,)\n",
      "transformer.ln_f.bias                                        (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Model parameters visualization\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('GPT-2 has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 1st Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 2nd Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[14:26]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 3rd Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[26:38]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-5:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
