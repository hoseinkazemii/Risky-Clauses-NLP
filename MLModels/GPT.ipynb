{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44145de0",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20d28036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5cdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Dataset.csv')\n",
    "texts = df['clause'].tolist()\n",
    "labels = df['risk'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9084b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a21d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64  # or any other suitable length\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
    "\n",
    "# Truncate and pad sequences\n",
    "padded_sequences = []\n",
    "for seq in tokenized_texts:\n",
    "    if len(seq) > max_length:\n",
    "        # Truncate the sequence if it exceeds max_length\n",
    "        seq = seq[:max_length]\n",
    "    else:\n",
    "        # Pad the sequence if it's shorter than max_length\n",
    "        seq = seq + [tokenizer.pad_token_id] * (max_length - len(seq))\n",
    "    padded_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23bdf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.LongTensor(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f40ee4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 'labels' for your texts\n",
    "dataset = TextClassificationDataset(padded_sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f1e1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels)\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97d79507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27dc2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT model and tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "# Use GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be1e164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze GPT model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be3e3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add classification head\n",
    "# class GPTClassifier(nn.Module):\n",
    "#     def __init__(self, gpt_model):\n",
    "#         super(GPTClassifier, self).__init__()\n",
    "#         self.gpt = gpt_model\n",
    "#         self.dropout = nn.Dropout(0.5)  # Add dropout layer with dropout rate 0.5\n",
    "#         self.fc = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         outputs = self.gpt(input_ids)[0]\n",
    "#         pooled_output = outputs[:, :, 0]  # Take the first token [CLS]\n",
    "#         pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "#         logits = self.fc(pooled_output)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Add classification head\n",
    "# class GPTClassifier(nn.Module):\n",
    "#     def __init__(self, gpt_model):\n",
    "#         super(GPTClassifier, self).__init__()\n",
    "#         self.gpt = gpt_model\n",
    "#         self.fc = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         outputs = self.gpt(input_ids)[0]\n",
    "\n",
    "#         pooled_output = outputs[:,:,0]  # Take the first token [CLS]\n",
    "#         logits = self.fc(pooled_output)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "class GPTClassifier(nn.Module):\n",
    "    def __init__(self, gpt_model, dropout_prob=0.5):\n",
    "        super(GPTClassifier, self).__init__()\n",
    "        self.gpt = gpt_model\n",
    "        self.fc = nn.Linear(25000, 1)  # Assuming hidden_state dimension is 25000\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.gpt(input_ids)[0]  # Getting hidden states\n",
    "        pooled_output = outputs[:, 0, :]  # Take the first token [CLS] \n",
    "        # print(\"pooled_output.shape\", pooled_output.shape)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # input_ids, labels = batch\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            predictions = (logits > 0).long()\n",
    "            correct_train += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, labels = batch\n",
    "                logits = model(input_ids)\n",
    "                loss = criterion(logits.squeeze(-1), labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                predictions = (logits > 0).long()\n",
    "                correct_val += (predictions == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        # Print training and validation statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "        # Store losses and accuracies for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "    \n",
    "\n",
    "def plot_val_acc_per_epoch(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "\n",
    "    # Plot losses\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracies\n",
    "    plt.plot(train_accuracies, label='Train Acc')\n",
    "    plt.plot(val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d57dedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier = GPTClassifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "416358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95ec5d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\PhD\\Papers\\NLP-Risky Clauses\\First Revision\\Comment2 Reviewer2-GPT\\GPT.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[39m=\u001b[39m train(classifier, train_dataloader, test_dataloader, optimizer, criterion, \u001b[39m10\u001b[39;49m, device)\n",
      "\u001b[1;32mc:\\PhD\\Papers\\NLP-Risky Clauses\\First Revision\\Comment2 Reviewer2-GPT\\GPT.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m logits \u001b[39m=\u001b[39m model(input_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), labels\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\PhD\\Papers\\NLP-Risky Clauses\\First Revision\\Comment2 Reviewer2-GPT\\GPT.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# print(\"pooled_output.shape\", pooled_output.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(pooled_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PhD/Papers/NLP-Risky%20Clauses/First%20Revision/Comment2%20Reviewer2-GPT/GPT.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(classifier, train_dataloader, test_dataloader, optimizer, criterion, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_acc_per_epoch(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8afce903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([32, 128])\n",
      "*************outputs shape***********:  torch.Size([14, 128])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the test dataset and generate predictions\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        logits = classifier(input_ids)\n",
    "        predictions = (logits > 0).long()  # Convert logits to binary predictions\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_labels.extend(predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "59e62e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (with 4 decimal points precision):\n",
      "[[313   7]\n",
      " [203   3]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the confusion matrix with 4 decimal points precision\n",
    "print(\"Confusion Matrix (with 4 decimal points precision):\")\n",
    "print(np.round(conf_matrix, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8a13c1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.98      0.75       320\n",
      "         1.0       0.30      0.01      0.03       206\n",
      "\n",
      "    accuracy                           0.60       526\n",
      "   macro avg       0.45      0.50      0.39       526\n",
      "weighted avg       0.49      0.60      0.47       526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 has 292 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (25000, 1024)\n",
      "transformer.wpe.weight                                   (256, 1024)\n",
      "\n",
      "==== 1st Transformer Layer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                  (1024,)\n",
      "transformer.h.0.ln_1.bias                                    (1024,)\n",
      "transformer.h.0.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.0.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.0.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.0.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.0.ln_2.weight                                  (1024,)\n",
      "transformer.h.0.ln_2.bias                                    (1024,)\n",
      "transformer.h.0.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.0.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.0.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.0.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== 2nd Transformer Layer ====\n",
      "\n",
      "transformer.h.1.ln_1.weight                                  (1024,)\n",
      "transformer.h.1.ln_1.bias                                    (1024,)\n",
      "transformer.h.1.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.1.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.1.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.1.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.1.ln_2.weight                                  (1024,)\n",
      "transformer.h.1.ln_2.bias                                    (1024,)\n",
      "transformer.h.1.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.1.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.1.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.1.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== 3rd Transformer Layer ====\n",
      "\n",
      "transformer.h.2.ln_1.weight                                  (1024,)\n",
      "transformer.h.2.ln_1.bias                                    (1024,)\n",
      "transformer.h.2.attn.c_attn.weight                      (1024, 3072)\n",
      "transformer.h.2.attn.c_attn.bias                             (3072,)\n",
      "transformer.h.2.attn.c_proj.weight                      (1024, 1024)\n",
      "transformer.h.2.attn.c_proj.bias                             (1024,)\n",
      "transformer.h.2.ln_2.weight                                  (1024,)\n",
      "transformer.h.2.ln_2.bias                                    (1024,)\n",
      "transformer.h.2.mlp.c_fc.weight                         (1024, 4096)\n",
      "transformer.h.2.mlp.c_fc.bias                                (4096,)\n",
      "transformer.h.2.mlp.c_proj.weight                       (4096, 1024)\n",
      "transformer.h.2.mlp.c_proj.bias                              (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.h.23.mlp.c_fc.bias                               (4096,)\n",
      "transformer.h.23.mlp.c_proj.weight                      (4096, 1024)\n",
      "transformer.h.23.mlp.c_proj.bias                             (1024,)\n",
      "transformer.ln_f.weight                                      (1024,)\n",
      "transformer.ln_f.bias                                        (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Model parameters visualization\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('GPT-2 has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 1st Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 2nd Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[14:26]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== 3rd Transformer Layer ====\\n')\n",
    "\n",
    "for p in params[26:38]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-5:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
